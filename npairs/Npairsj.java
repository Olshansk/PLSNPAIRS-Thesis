package npairs;

import npairs.shared.matlib.*;
import extern.niftijlib.Nifti1Dataset;

import java.io.FileNotFoundException;
import java.io.IOException;
import java.io.ObjectOutputStream;
import java.io.PrintStream;

import npairs.utils.CVA;
import npairs.utils.PCA;
import npairs.utils.PredictionStats;
import npairs.utils.Resampler;
import npairs.io.NiftiIO;
import npairs.io.NpairsjIO;
import pls.shared.MLFuncs;
import npairs.utils.ZScorePatternInfo;
import npairs.io.NpairsDataLoader;

import java.io.*;
import java.util.*;
import java.util.List;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileStatus;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;

import npairs.Test;

// Start imports for XML parsing (determine number of slaves dynamically)
import javax.xml.parsers.DocumentBuilderFactory;
import javax.xml.parsers.DocumentBuilder;
import javax.xml.parsers.ParserConfigurationException;

import org.w3c.dom.Document;
import org.w3c.dom.NodeList;
import org.w3c.dom.Node;
import org.w3c.dom.Element;
import org.xml.sax.SAXException;

import com.jcraft.jsch.Channel;
import com.jcraft.jsch.ChannelExec;
import com.jcraft.jsch.JSch;
import com.jcraft.jsch.Session;

import java.io.File;
import java.io.FileNotFoundException;
import java.io.FileReader;
import java.io.IOException;
import java.io.BufferedReader;
import java.io.InputStream;
import java.io.InputStreamReader;
// End imports for XML parsing
import java.io.PrintWriter;

import java.net.InetAddress;

import com.jcraft.jsch.*;
import java.awt.*;
import javax.swing.*;
import java.io.*;

import java.util.concurrent.*;
import java.util.*;

import org.apache.commons.io.*;

public class Npairsj {

	boolean debug = false;
	/***************************************************************************************
	 * class Npairsj() // Main entry point for NPAIRS program
	 * 
	 * @author anita oder
	 * 
	 **************************************************************************************/

	NpairsjSetupParams setupParams; // Contains all param info entered by user.

	String matlibType; // Which matrix library to use; current choices
						// are 'MATLAB' or 'COLT'(not case sensitive).

	int[][][] splits; // Contains info about which split objects belong to each
						// split half when resampling data. Dims of 'splits' are
						// [2][numSamples][]; splits[0] contains info for first
						// split
						// half and splits[1] contains info for second split
						// half, and
						// numSamples = min(setupParams.numSplits, max poss
						// splits).
						// Array elements are indices into
						// setupParams.splitObjectLabels
						// array (in ascending order).
						//

	NpairsDataLoader dataLoader; // Contains Npairs data, feat-selected data and
									// inverse feature-
									// selected data Matrices.

	/* Results: */

	Analysis fullDataAnalysis = null; // contains results from full-data
										// analysis, if run

	public CVA fullDataCVA; // contains CVA results from full-data analysis, if
							// run

	CVA splitDataCVA1; // contains CVA results from first half split-data
						// analysis,
						// if run (holds temp results from each split analysis
						// as it's run)

	CVA splitDataCVA2; // contains CVA results from second half split-data
						// analysis,
						// if run (holds temp results from each split analysis
						// as it's run)

	public PCA fullDataPCA; // contains PCA results from full-data analysis, if
							// run

	PCA splitDataPCA1; // contains PCA results from first half split-data
						// analysis,
						// if run (holds temp results from each split analysis
						// as it's run)

	PCA splitDataPCA2; // contains PCA results from second half split-data
						// analysis,
						// if run (holds temp results from each split analysis
						// as it's run)

	Matrix split1CVAEvals; // contains eigenvalues for each first split half
							// (numSamples
							// of them) and each CV dimension; dims = numSamples
							// rows
							// X num CV dims cols

	Matrix split2CVAEvals; // contains eigenvalues for each second split half
							// (numSamples
							// of them) and each CV dimension; dims = numSamples
							// rows
							// X num CV dims cols

	public double[] avgSplit1CVAEvals; // contains avg CV evals across first
										// split half CVA results;
										// length of array = num CV dims
	public double[] avgSplit2CVAEvals; // contains avg CV evals across second
										// split half CVA results;
										// length of array = num CV dims

	public Matrix avgCVScoresTrain; // contains avg cv scores across all
									// training sets;
									// dims = num input data vols X num cv dims

	public Matrix avgCVScoresTest; // contains avg cv scores across all test
									// sets, i.e.,
									// avg of cv scores generated by projecting
									// test data onto
									// corresponding training cv eigenimages;
									// dims = num input data vols X num cv dims

	private Matrix[] r2; // contains r^2 values calculated for each split half
							// between CV scores and data input into CV (e.g. PC
							// scores
							// representing data in PC space)

	Matrix corrCoeffs; // contains correlation coefficients for each split
						// between eigenimages from each split half analysis;
						// dims are: numSamples rows (see 'splits' above) by
						// num dims in eigenimage results for model
						// to be summarized (e.g. CV dims)
						// (ONLY EXISTS if split 2 models are generated, i.e.,
						// if training and test sets are switched.)

	Matrix noisePattStdDev; // contains standard deviation of data from both
							// split
							// halves projected onto 'noise' axis of scatterplot
							// between split halves
							// dims are same as for corrCoeffs above
							// (ONLY EXISTS if split 2 models are generated,
							// i.e.,
							// if training and test sets are switched.)
	/* Prediction stats */

	// TODO: consider better storage strategy for prediction stats (ppAllClasses
	// ordering of dims
	// inconsistent with rest of prediction variables; storing correctPred as
	// array of double
	// Matrices is unwieldy)

	public Matrix[] ppTrueClass; // for each test volume, contains posterior
									// probabilities
									// (with and without priors)
									// of belonging to true class
									// (1 Matrix for each split half)
									// 1st row of each Matrix = probs with
									// priors
									// 2nd row "   " " = probs without priors

	public Matrix[] sqrdPredError; // contains squared prediction error
									// (1-ppTrueClass)^2
									// (with and without priors)
									// for each test volume
									// (1 Matrix for each split half)
									// 1st row of each Matrix = SPE with priors
									// 2nd row "   " " = SPE without priors

	public Matrix[] predClass; // contains predicted class for each test volume
								// (with and without priors)
								// (1 Matrix for each split half)
								// 1st row of each Matrix = probs with priors
								// 2nd row "   " " = probs without priors

	public Matrix[] correctPred; // contains labels for each test volume
									// indicating
									// whether predicted class is true class:
									// 1.0 = true; 0.0 = false.
									// (1 Matrix for each split half)
									// 1st row of each Matrix = probs with
									// priors
									// 2nd row "   " " = probs without priors

	public Matrix[][] ppAllClasses; // contains posterior probs (with and
									// without priors)
									// for each test volume of belonging to each
									// class
									// ppAllClasses[i][j] =
									// (no. test vols) X (no. classes) Matrix
									// for j = 0 (priors) or 1 (no priors);
									// ith split half

	public Matrix[][] ppTrueClass_temp;
	public Matrix[][] sqrdPredError_temp;
	public Matrix[][] predClass_temp;
	public Matrix[][] correctPred_temp;
	public Matrix[][][] ppAllClasses_temp;
	public Matrix[] noisePattStdDev_temp;
	public Matrix[] corrCoeffs_temp;
	private Matrix[][] r2_temp;

	/* Summary results: */

	public Matrix avgSpatialPattern; // contains spatial pattern average across
										// all training
										// datasets for analysis step that is to
										// be
										// summarized (e.g. CVA results in PCA +
										// CVA analysis, or
										// PCA results if only PCA run)
										// No. of training sets: 2 * # splits in
										// split-half X-validation;
										// # input vols in bootstrap

	public Matrix avgZScorePattern; // contains average rSPM(z) [normalized
									// proj. of split
									// half patterns
									// onto 'signal axis'] across all splits for
									// model
									// to be summarized
									// (ONLY EXISTS if split 2 models are
									// generated, i.e. if
									// training and test sets are switched.)

	Matrix avgNoisePattern; // contains average noise pattern [normalized proj.
							// of
							// split half patterns onto 'noise axis'] across all
							// splits for model to be summarized
							// (ONLY EXISTS if split 2 models are generated,
							// i.e. if
							// training and test sets are switched.)

	// TODO: calculate and save Npairsj.pattHistograms,
	// Npairsj.zScorePattHistograms,
	// Npairsj.summInfluence

	Matrix pattHistograms; // contains histograms for spatial patterns generated
							// from each training set (only for analysis step
							// that
							// is to be summarized)
							// ** will worry about details of generating this
							// stat
							// later; implementation shown here will probably
							// need
							// tweaking-- should be OK in terms of code
							// structure,
							// though **

	Matrix zScorePattHistograms; // contains histograms for rSPM(z) patterns
									// from each
									// training set (only for analysis step that
									// is to be
									// summarized)
									// ** will worry about details of generating
									// this stat
									// later; implementation shown here will
									// probably need
									// tweaking-- should be OK in terms of code
									// structure,
									// though **

	Matrix summInfluence; // contains influence metrics for each Subject/Session
							// ** will worry about details of generating this
							// stat
							// later ** (not implemented yet here)
	private boolean reshapedR2 = false; // set to true if r2 has been reshaped
										// to final summary
										// format (where there are nCVDims
										// Matrices, each with
										// dims nSplitAnalyses X no. data dims
										// (e.g. PC dims)
										// input into CVA split analyses).
	public boolean computeR2 = true; // always save r2 info (unless doing
										// JavaQuadCVA)

	public static PrintStream output = System.out; // for directing all output
													// statements

	// (to e.g. console or log file);
	// initialized in NpairsjSetupParams.initLogFile()

	// The calculate (p,r) values acquired when NPAIRS is finished running 
	public double predictability = -1;
	public double reproducibility = -1;
	
	/**************************************************************************************/

	/***************************************************************************************
	 * Npairsj constructor takes either setup params .mat file or
	 * NpairsjSetupParams object:
	 * 
	 * @param dataLoader
	 * @param paramMatFileName
	 * @param matlibType
	 * @param eventRelAnalysis
	 * @throws NpairsjException
	 * @throws IOException
	 **************************************************************************************/

	public Npairsj(NpairsDataLoader dataLoader, String paramMatFileName,
			String matlibType, boolean eventRelAnalysis)
			throws NpairsjException, IOException {
		this(dataLoader, new NpairsjSetupParams(paramMatFileName, false),
				matlibType);
	}

	public Npairsj(NpairsDataLoader dataLoader, NpairsjSetupParams nsp,
			String matlibType) throws NpairsjException, IOException {

		this.matlibType = matlibType;
		this.setupParams = nsp;
		this.dataLoader = dataLoader;
		// this.computeR2 = nsp.saveLotsOfFiles;
		nsp.useQuadCVA = false; // Added by Grigori
		if (nsp.useQuadCVA)
			computeR2 = false; // not implemented for quad cva

		// TODO: fix saveListfile to work for updated NpairsjSetupParams
		// dataFileNames and maskFileNames variables
		// if (setupParams.saveLotsOfFiles && setupParams.cvaRun) {
		// if (!setupParams.dataIs4D) {
		// try {
		// // save listfile (for IDL NPAIRS)
		// // (needed to view cv scores plot using J. Anderson's idl tool
		// // 'cva_plot_cvs.pro')
		// setupParams.saveListfile();
		// }
		// catch (IOException e) {
		// throw new NpairsjException("Error saving listfile");
		// }
		// }
		// }

		double sTime = System.currentTimeMillis();

		// TODO: do preprocessing once here instead of every time an Analysis is
		// run
		// if (setupParams.preProcess) {
		// if (setupParams.removeSessionMeans) {
		// data = removeMeanSessionScans(setupParams.sessionLabels, data);
		// }
		// }

		runAnalysis();
		double tTime = (System.currentTimeMillis() - sTime) / 1000;

		int hr = (int) (tTime / 3600);
		int min = (int) ((tTime / 60) - (hr * 60));
		double s = tTime - (hr * 3600) - (min * 60);
		output.print("Total time running NPAIRS analysis: " + hr + " h " + min
				+ " min ");
		// Alan:
		System.out.println("Total time running NPAIRS analysis: " + hr + " h "
				+ min + " min ");

		output.printf("%.3f", s);
		output.println(" s");
		
		predictability = getPredictionValue();
		reproducibility = getReproducibilityValue();
				
		System.out.println("Final predictability: " + predictability);
		System.out.println("Final reproducibility: " + reproducibility);

	}

	private void runAnalysis() throws NpairsjException, IOException {

		// Full Data Analysis:
		if (setupParams.runFullDataAnalysis) {

			output.println("Running full-data analysis...");
			double sTime = System.currentTimeMillis();

			runFullDataAnalysis();

			double tTime = (System.currentTimeMillis() - sTime) / 1000;
			output.println("Finished running full data analysis [" + tTime
					+ " s]");

		}

		// Split Analyses and Stats Generation:

		if (setupParams.resampleData) {
			// TODO: verify whether full-data reference analysis is really
			// always
			// required when resampling data
			// /if(false){
			if (!setupParams.runFullDataAnalysis) {
				throw new NpairsjException(
						"Must run full-data reference analysis "
								+ "before resampling data");
			}

			createSplits(true);
			saveSplitVolInfo();

			output.println("Running split analyses...");
			double sTime = System.currentTimeMillis();

			runSplitAnalyses();

			double tTime = (System.currentTimeMillis() - sTime) / 1000;
			output.println("Finished running split analyses [" + tTime + " s]");

			saveSummarySplitResults();
		}
	}

	private void saveSummarySplitResults() throws IOException, NpairsjException {
		// Always save true class post probs with priors in text file as 2D
		// array.
		// If saveLotsOfFiles, save all summary prediction results in
		// ASCII/Analyze format.
		output.println("Saving prediction stats...");
		savePredictionStats(setupParams.saveLotsOfFiles);

		// Always save reproducibility stats in a textfile, too.
		output.println("Saving reproducibility stats... ");
		saveCorrCoeffs("IDL");

		// And R2 output...
		output.println("Saving r2 values for each split analysis...");
		saveSummaryR2("IDL");

		if (setupParams.saveLotsOfFiles) {

			output.println("Saving average CV Scores (training and test)...");
			saveAvgCVScores("IDL");

			output.println("Saving average spatial pattern... ");
			saveAvgSpatPattern("IDL");

			// if (computeR2) {
			// output.println("Saving r2 values for each split analysis...");
			// saveSummaryR2("IDL");
			// }

			if (setupParams.switchTrainAndTestSets) {

				output.println("Saving rSPM{Z}... ");
				saveZScoreAvgPatt("IDL");

				output.println("Saving average noise pattern... ");
				saveNoiseAvgPatt("IDL");

				output.println("Saving noise pattern stats... ");
				saveNoiseStdDev("IDL");
			}
		}
	}

	/**
	 * Saves summary r2 file across all splits for each CV Dim, not the
	 * individual r2 files that can be saved for each split analysis using
	 * CVA.saveCVAResultsIDL by setting boolean saveR2 input arg to true
	 * 
	 * @param format
	 */
	private void saveSummaryR2(String format) throws NpairsjException {
		if (!reshapedR2) {
			throw new NpairsjException(
					"Must reshape R2 Matrix array into summary "
							+ "format before saving.");
		}
		if (format.toUpperCase().equals("IDL")) {
			int nCVDims = r2.length;
			for (int cv = 0; cv < nCVDims; ++cv) {
				String r2SplitsFile = setupParams.resultsFilePrefix
						+ ".CVA.SUMM.CVDIM_" + cv + ".r2";
				r2[cv].printToFile(r2SplitsFile, format);
			}
		} else {
			throw new IllegalArgumentException("Output format \'" + format
					+ "\' not " + "implemented.");
		}
	}

	private void runFullDataAnalysis() throws NpairsjException, IOException {
		if (setupParams.initFeatSelect) {
			fullDataAnalysis = new Analysis(dataLoader.getFeatSelData(),
					setupParams);
		} else
			fullDataAnalysis = new Analysis(dataLoader.getOrigData(),
					setupParams);

		fullDataAnalysis.run();

		fullDataPCA = fullDataAnalysis.getPCA(); // null if pca not run
		fullDataCVA = fullDataAnalysis.getCVA(); // null if cva not run

		if (setupParams.initFeatSelect) {
			// rotate results back to original space from
			// initial feature selection space
			if (setupParams.pcaRun) {
				if (setupParams.pcEigimsToBigSpace) {
					// TODO: replace cvaPCSetAll with generalized PC dims
					// (since if CVA not run, won't have cva PC set)
					output.print("Transforming PCA back into orig. space... ");
					double sTime = System.currentTimeMillis();
					fullDataPCA.rotateEigimsToOrigSpace(
							setupParams.cvaPCSetAll,
							dataLoader.getEVDProjFactorMat(),
							dataLoader.getOrigData());
					double tTime = (System.currentTimeMillis() - sTime) / 1000;
					output.println("[" + tTime + " s]");

				}
			}
			if (setupParams.cvaRun) {
				double sTime = System.currentTimeMillis();
				if (debug) {
					output.print("Transforming CVA back into orig. space... ");
				}
				fullDataCVA.rotateEigimsToOrigSpace(
						dataLoader.getEVDProjFactorMat(),
						dataLoader.getOrigData());
				if (debug) {
					double tTime = (System.currentTimeMillis() - sTime) / 1000;
					output.println("[" + tTime + " s]");
				}
			}
		}
		saveFullDataResults();
	}

	private void saveFullDataResults() throws IOException, NpairsjException {
		if (setupParams.saveLotsOfFiles && setupParams.saveFullDataAnalysis) {
			if (setupParams.pcaRun) {
				// if (debug) {
				output.println("Saving full-data PCA results...");
				// }
				String pcaSavePref = setupParams.resultsFilePrefix;
				if (!setupParams.pcEigimsToBigSpace) {
					pcaSavePref += ".InitFSpace";
				}
				fullDataPCA.savePCAResultsIDL(pcaSavePref, null, false, 0, 0);

			}

			if (setupParams.cvaRun) {
				// if (debug) {
				output.println("Saving full-data CVA results...");
				// }
				fullDataCVA.saveCVAResultsIDL(setupParams.resultsFilePrefix,
						false, 0, 0, true);
			}
		}

		if (setupParams.saveDataPostPCA) {
			// save denoised (i.e. PCA dim-reduced) input data
			// (in orig img space)
			output.print("Saving denoised (post-PCA) image data... ");
			double sTime = System.currentTimeMillis();
			if (!setupParams.pcEigimsToBigSpace) {
				// still need to project eigims into big space
				// before saving denoised data
				fullDataPCA.rotateEigimsToOrigSpace(setupParams.cvaPCSetAll,
						dataLoader.getEVDProjFactorMat(),
						dataLoader.getOrigData());
			}
			fullDataPCA.saveDataPostPCA(setupParams, dataLoader);
			double tTime = (System.currentTimeMillis() - sTime) / 1000;
			output.println("[" + tTime + " s]");
		}
	}

	/*
	 * The dfs.replication property in /hdfs-site.xml is supposed to equal the
	 * number of slaves in the hadoop job.
	 * 
	 * This method determines the number of slaves by firstly identifying the
	 * hadoop environment variable to get the location of the hdfs-site.xml
	 * file. Depending on the hadoop version runnig (<2.x or >=2.x), the file is
	 * located in different directories so that must also be checked. Once a
	 * full path to the file is determined, the XML file is parsed to determine
	 * the replication info.
	 */
	public static String getHadoopHomeDir() {
		try {
			String hadoop_loc = "";
		
			hadoop_loc = System.getenv().get("HADOOP_HOME");
		
			if (hadoop_loc.length() == 0) {
				hadoop_loc = System.getenv().get("HADOOP_PREFIX");
			}
		
			if (hadoop_loc.length() == 0) {
				throw new Exception("Hadoop environment variables not found.");
			}
		
			Process process = Runtime.getRuntime().exec("hadoop version");
			BufferedReader stdInput = new BufferedReader(new InputStreamReader(process.getInputStream()));
			String hadoop_version = stdInput.readLine().split(" ")[1];
		
			String hadoop_conf_dir = "";
			
			if (hadoop_version.charAt(0) == '2') {
				hadoop_conf_dir = hadoop_loc + "/etc/hadoop/";
			} else {
				hadoop_conf_dir = hadoop_loc + "/etc/conf/";
			}
		
			if (hadoop_conf_dir.length() == 0) {
				throw new Exception("Hadoop version cannot be identified.");
			}
			
			return hadoop_conf_dir;
			
		} catch (Exception e) {
			e.printStackTrace();
			System.exit(-1);
		}
		
		return "";

	}
	
	public static int getNumAvailableSlaves() {
		try {
			
			String hadoop_home_dir = getHadoopHomeDir();
			String hdfs_loc = hadoop_home_dir + "hdfs-site.xml";
			String slaves_loc = hadoop_home_dir + "slavesl";
			
			int slaves = 0;
			int hdfs = -1;

			BufferedReader reader = new BufferedReader(new FileReader(
					slaves_loc));
			String line;
			while ((line = reader.readLine()) != null) {
				if (line.length() != 0) {
					slaves++;
				}
			}

			File fXmlFile = new File(hdfs_loc);
			DocumentBuilderFactory dbFactory = DocumentBuilderFactory
					.newInstance();
			DocumentBuilder dBuilder = dbFactory.newDocumentBuilder();
			Document doc = dBuilder.parse(fXmlFile);
			doc.getDocumentElement().normalize();
			NodeList nList = doc.getElementsByTagName("property");
			for (int i = 0; i < nList.getLength(); i++) {
				Node nNode = nList.item(i);
				if (nNode.getNodeType() == Node.ELEMENT_NODE) {
					Element eElement = (Element) nNode;
					if (eElement.getElementsByTagName("name").item(0)
							.getTextContent().equals("dfs.replication")) {
						hdfs = Integer.parseInt(eElement
								.getElementsByTagName("value").item(0)
								.getTextContent());
					}
				}
			}
			if (hdfs == -1) {
				throw new Exception(
						"dfs.replication is not specified in hdfs-site.xml.");
			}

			if (slaves == hdfs) {
				return slaves;
			} else {
				throw new Exception(
						"dfs.replication is not specified in hdfs-site.xml.");
			}
		} catch (Exception e) {
			System.out.println("Exception getting the available number of slaves.");
			e.printStackTrace();
			System.exit(-1);
		}
		return -1;
	}

	public static int getNumAvailableMappersFromFileName(String fileName) {
		try {	
			System.out.println("Trying to get mapper number from: " + fileName);
			int numMappers = -1;
			DocumentBuilderFactory dbFactory = DocumentBuilderFactory.newInstance();
			DocumentBuilder dBuilder = dbFactory.newDocumentBuilder();
			Document doc = dBuilder.parse(new File("./" + fileName));
			doc.getDocumentElement().normalize();
			NodeList nList = doc.getElementsByTagName("property");
			for (int i = 0; i < nList.getLength(); i++) {
				Node nNode = nList.item(i);
				if (nNode.getNodeType() == Node.ELEMENT_NODE) {
					Element eElement = (Element) nNode;
					if (eElement.getElementsByTagName("name").item(0).getTextContent().equals("mapreduce.tasktracker.map.tasks.maximum")) {
						numMappers = Integer.parseInt(eElement.getElementsByTagName("value").item(0).getTextContent());
					}
				}
			}
			
			return numMappers;
		} catch (IOException e) {
			System.out.println("IOException");
			e.printStackTrace();
			System.exit(-1);
		} catch (SAXException e) {
			System.out.println("SAXException");
			System.exit(-1);
		} catch (IllegalArgumentException e) {
			System.out.println("IllegalArgumentException");
			System.exit(-1);
		} catch (ParserConfigurationException e) {
			System.out.println("ParserConfigurationException");
			System.exit(-1);
		}
		
		return -1; 
	}
	
	public static int getNumAvailableMappers() {
		String hadoop_home_dir = getHadoopHomeDir();
		String map_loc = hadoop_home_dir + "mapred-site.xml";
		return getNumAvailableMappersFromFileName(map_loc);
	}
	
	public static void sshExecute(String host, String user, String command) {
		try {
			JSch jsch = new JSch();

			String userHome = System.getProperty("user.home");
			jsch.setKnownHosts(userHome + "/.ssh/known_hosts");
			jsch.addIdentity(userHome + "/.ssh/id_rsa");

			Session session = jsch.getSession(user, host, 22);
			session.connect();

			Channel channel = session.openChannel("exec");

			((ChannelExec) channel).setCommand(command);

			// channel.setInputStream(System.in);
			channel.setInputStream(null);

			// channel.setOutputStream(System.out);

			// FileOutputStream fos=new
			// FileOutputStream("/tmp/stderr");
			// ((ChannelExec)channel).setErrStream(fos);
			((ChannelExec) channel).setErrStream(System.err);

			InputStream in = channel.getInputStream();

			channel.connect();

			byte[] tmp = new byte[1024];
			while (true) {
				while (in.available() > 0) {
					int i = in.read(tmp, 0, 1024);
					if (i < 0) {
						break;
					}
					System.out.print(host + new String(tmp, 0, i));
				}
				if (channel.isClosed()) {
					System.out.println(host + "exit-status: "
							+ channel.getExitStatus());
					break;
				}
				try {
					Thread.sleep(1000);
				} catch (Exception ee) {

				}
			}
			channel.disconnect();
			session.disconnect();
		} catch (Exception e) {
			System.out.println(e);
		}
	}
	
	/*
	 * This method returns an array of strings of all the machines on which this
	 * 'master' node can execute independent hadoop jobs.
	 * 
	 * Note that this file is independent of hadoop configuration files and must
	 * be present in the same directory from which the jar is executed called
	 * "slaves".
	 */
	public static String[] getListOfSlaves() {
		ArrayList<String> hostArray = new ArrayList<String>();
		try {

			String localDirectory = System.getProperty("user.dir");
			String slavesFileName = localDirectory + "/slaves";

			BufferedReader reader = new BufferedReader(new FileReader(
					slavesFileName));
			String line;
			while ((line = reader.readLine()) != null) {
				if (line.length() != 0) {
					hostArray.add(line);
				}
			}

		} catch (Exception e) {
			System.out.println("Could not find the slaves file in current direcotry.");
			e.printStackTrace();
		}
		return hostArray.toArray(new String[hostArray.size()]);
	}

	private void createParentDirectoryIfNecessary(File file) {
		File parent_directory = file.getParentFile();
		if (null != parent_directory) {
			parent_directory.mkdirs();
		}
	}

	private void writeStringToFile(String outString, String fileName) throws IOException {
		File file = new File(fileName);
		createParentDirectoryIfNecessary(file);
		BufferedWriter out = new BufferedWriter(new FileWriter(file));
		out.write(outString);
		out.close();
	}

	private HashMap<String, Double> getRelativeSlaveEfficiencies(String [] slaves) throws IOException, NpairsjException {
		HashMap<String, Double> efficiencyMap = new HashMap<String, Double>();
		HashMap<String, Double> timeMap = new HashMap<String, Double>();
		double total = 0;

		File dir = new File("efficiencies");
		for (File child : dir.listFiles()) {
			BufferedReader in = new BufferedReader(new FileReader(child));
			String host = child.getName();
			String sTime = in.readLine();

			System.out.println(host + " " + sTime);

			double time = -1;
			try {
				time = Double.parseDouble(sTime);
			} catch (NumberFormatException e) {
				in.close();
				continue;
			}
			total += time;
			timeMap.put(host, time);
			in.close();
		}

		Set <String> keys = timeMap.keySet();
		if (keys.size() == 0) {
			throw new NpairsjException("Error. No host mapper efficiency files were found.");
		} else if (keys.size() == 1) {
			System.out.println("Only one host was found");
			String key = (String) keys.iterator().next();
			double fractionWork = 1.0;
			efficiencyMap.put(key, fractionWork);
			System.out.println(key + " " + fractionWork);
		} else {
			System.out.println("Multiple hosts were found");
			for (String key : timeMap.keySet()) {
				double fractionTime = timeMap.get(key) / total;
				double fractionWork = 1.0 - fractionTime; // because less time => faster machine
				efficiencyMap.put(key, fractionWork);
				System.out.println(key + " " + fractionWork);
			}
		}

		return efficiencyMap;
	}

	private void removeUselessSlaves(HashMap<String, Double> origMap,int numSamples) {
		Iterator<Map.Entry<String, Double>> it = origMap.entrySet().iterator();
		double totalRemoved = 0;
		while (it.hasNext()) {
			Map.Entry<String, Double> entry = it.next();
			double fractionWork = entry.getValue();
			int numSamplesPerSlave = (int) Math.round(fractionWork
					* ((double) numSamples));
			System.out.println(numSamplesPerSlave);
			if (numSamplesPerSlave == 0) {
				it.remove();
				totalRemoved += fractionWork;
			}
		}

		double addPerSlaveRemaining = totalRemoved / origMap.size();
		for (String key : origMap.keySet()) {
			origMap.put(key, origMap.get(key) + addPerSlaveRemaining);
		}
	}

	private void serializeSerObjects() {
		double sTime = System.currentTimeMillis();

		System.out.println("starting to serialized fullDataAnalysis object");
		try {
			ObjectOutput out = new ObjectOutputStream(new FileOutputStream(
					"fullDataAnalysis.ser"));
			out.writeObject(fullDataAnalysis);
			out.close();
		} catch (IOException ex) {
			ex.printStackTrace();
		}

		System.out.println("starting to serialized setupParams object");
		try {
			ObjectOutput out = new ObjectOutputStream(new FileOutputStream(
					"setupParams.ser"));
			out.writeObject(setupParams);
			out.close();
		} catch (IOException ex) {
			ex.printStackTrace();
		}

		System.out.println("starting to serialized splits object");
		try {
			ObjectOutput out = new ObjectOutputStream(new FileOutputStream("splits.ser"));
			out.writeObject(splits);
			out.close();
		} catch (IOException ex) {
			ex.printStackTrace();
		}
		/*
		 * System.out.println("starting to serialized dataloader object");
		 * 
		 * try { ObjectOutput out = new ObjectOutputStream(new
		 * FileOutputStream("dataLoader.ser")); out.writeObject(dataLoader);
		 * out.close(); } catch(IOException ex) { ex.printStackTrace(); }
		 */
		double tTime = (System.currentTimeMillis() - sTime) / 1000;
		System.out.println("Serialization takes: " + tTime + "seconds in total");
	}

	/**
	 * Runs split analyses and accumulates summary split results
	 * 
	 * @throws Exception
	 */
	
	private void deleteHadoopInputFiles(String [] slaves) {
		// Delete any existing directories using for the input files of each slave
		for (String slave: slaves) {
			FileUtils.deleteQuietly(new File(slave));
		}
	}
	
	private void copyHadoopInputFilesToHDFS(FileSystem fs, String [] slaves) throws IOException {
		Path hdfs = new Path(Test.hadoopDirectory);
		for (String slave : slaves) {
			Path hadoop_slave = new Path(Test.hadoopDirectory + slave);
			deleteDirectoryInFileSystem(fs, hadoop_slave);
			fs.copyFromLocalFile(false, new Path(slave), hdfs);
		}
	}
	
	private static void deleteDirectoryInFileSystem(FileSystem fs, Path path) throws IOException {
		if (fs.exists(path)) {
			fs.delete(path, true);
		}
	}
	
	public static void deleteAndRemakeDirectoryInFileSystem(FileSystem fs, Path path) throws IOException{
		deleteDirectoryInFileSystem(fs, path);
		fs.mkdirs(path);
	}
	
	private void executeHadoop(String [] slaves) {
		ExecutorService service = Executors.newCachedThreadPool();		
		List<Future<?>> futures = new ArrayList<Future<?>>();
		for (final String slave : slaves) {
			Runnable runnable = new Runnable() {
				public void run() {
					sshExecute(slave, "hduser", "cd /home/hduser/NPAIRS_Files/NPAIRS/; /usr/local/hadoop/bin/hadoop jar npairs_hadoop.jar "+ slave + " out_" + slave);
				}
			};
			futures.add((Future<?>) service.submit(runnable));
		}

		service.shutdown();
		
		for (Future<?> future : futures) {
			try {
				future.get();
			} catch (InterruptedException e) {
				e.printStackTrace();
			} catch (ExecutionException e) {
				e.printStackTrace();
			}
		}
	}
	
	// Assumes that if the file exists, then it contains good data
	private boolean checkIfSlaveEfficienciesAvailable(String [] slaves) {
		File dir = new File("efficiencies");
		for (File child : dir.listFiles()) {
			try {
				BufferedReader in = new BufferedReader(new FileReader(child));
				in.close();
			} catch(FileNotFoundException e) {
				//if child does not exist, is a directory rather than a regular file, or for some other reason cannot be opened for reading.
				return false; 
			} catch (IOException e) {
				return false;
			}
			
		}
		return true;
	}
	
	private void runSplitAnalyses() throws IOException, NpairsjException {
		int numSamples = splits[0].length; // numSamples ==
											// min(setupParams.numSplits, max
											// num splits)
		System.out.println("No. splits: " + numSamples);

		int totalNumSplitAnalyses = numSamples;
		if (setupParams.switchTrainAndTestSets) {
			totalNumSplitAnalyses = 2 * numSamples;
		}

		initPredStats(totalNumSplitAnalyses);
		if (computeR2) {
			initR2Results(totalNumSplitAnalyses);
		}

		if (setupParams.cvaRun) {
			initCVASplitResults(numSamples);
		}

		int[] vCountTr = new int[setupParams.numVols]; // each element
														// incremented whenever
														// corresp.
														// vol (row of input
														// data) incl. in sample
														// training data
		int[] vCountTe = new int[setupParams.numVols]; // each element
														// incremented whenever
														// corresp.
														// vol incl. in sample
														// test data

		int numAnalyses = 0;

		// Create a reference ot the local and hadoop directory
		Path local = new Path(Test.localDirectory);
		Path hdfs = new Path(Test.hadoopDirectory);
		
		// Serialize objects
		serializeSerObjects();

		// Copy data to hdfs thatll be require for efficiency testing and for the actual analysis 			
		FileSystem hdfsFileSystem = FileSystem.get(new Configuration());

		Npairsj.deleteAndRemakeDirectoryInFileSystem(hdfsFileSystem, hdfs);
		
		Path localSetupParams = new Path("setupParams.ser");
		hdfsFileSystem.copyFromLocalFile(false, localSetupParams, hdfs);

		Path localSplits = new Path("splits.ser");
		hdfsFileSystem.copyFromLocalFile(false, localSplits, hdfs);

		Path localfullDataAnalysis = new Path("fullDataAnalysis.ser");
		hdfsFileSystem.copyFromLocalFile(false, localfullDataAnalysis, hdfs);

	    Path dataLoader = new Path("dataLoader.ser");
	    hdfsFileSystem.copyFromLocalFile(false, dataLoader, hdfs);
		
		// Get the list and numberof slaves
		String[] slaves = getListOfSlaves();
		int numSlaves = slaves.length;
		System.out.println("Number of available slaves: " + numSlaves);

		// Delete old input files if they exist
		deleteHadoopInputFiles(slaves);		
		
		// Create a path to the output directory and delete it in case it exists
		Path hdfs_out = new Path(Test.hadoopDirectory + "out_npairsj_ser");
		deleteAndRemakeDirectoryInFileSystem(hdfsFileSystem, hdfs_out);
		
		boolean efficienciesAvailable = checkIfSlaveEfficienciesAvailable(slaves);
		if (!efficienciesAvailable) {
			// Create a path to store the efficincies
			Path efficienciesPath = new Path(Test.hadoopDirectory, "efficiencies");
			deleteAndRemakeDirectoryInFileSystem(hdfsFileSystem, efficienciesPath);
			FileUtils.deleteQuietly(new File("efficiencies"));
			
			// Create 1 mapper for each slave thats only responsbile for the 0th split
			for (String slave : slaves) {
				writeStringToFile("0", slave + "/0");
			}
			
			// Copy hadoop input files over to hdfs
			copyHadoopInputFilesToHDFS(hdfsFileSystem, slaves);
			
			// Execute efficiency testing
			System.out.println("Starting efficiency testing...");
			executeHadoop(slaves);
			System.out.println("relative efficiencies calculated");
			
			// Copy the efficines to the local path
			hdfsFileSystem.copyToLocalFile(true, efficienciesPath, local);
		} else {
			System.out.println("Don't need to calculate efficiencies.");
		}
		
		// Retrieve relative efficiencies
		HashMap<String, Double> relativeSlaveEfficiencies = getRelativeSlaveEfficiencies(slaves);
		
		// Determine the number of mappers per machine
		HashMap<String, Integer> mappersPerHost = new HashMap<String, Integer>();
		for (String slave: slaves) {
			String fileName = "mapred-site-" + slave + ".xml";
			String command = "scp hduser@" + slave + ":/usr/local/hadoop/etc/hadoop/mapred-site.xml " + fileName; 
			Process p =  Runtime.getRuntime().exec(command);
			try {
			p.waitFor();
			} catch (InterruptedException e) {
				System.out.println("Process interrupted while copying mapper file.");
				System.exit(-1);
			}
			mappersPerHost.put(slave, getNumAvailableMappersFromFileName(fileName));
			File file = new File(fileName);
			file.delete();
		}
		System.out.println(mappersPerHost);
		
		// Determine the total number of mappers available
		int numMappers = 0;
		for (String key : mappersPerHost.keySet()) {
			numMappers += mappersPerHost.get(key);
		}
		System.out.println("Total mappers available: " + numMappers);
		
		// Allocate the array to store the indexes for each mapper in the future
		String[] indexes = new String[numMappers];
		
		// Determine the total slave weight
		double totalSlavesWeight = 0;
		for (String key : mappersPerHost.keySet()) {
			totalSlavesWeight += ((double)mappersPerHost.get(key)) * relativeSlaveEfficiencies.get(key) ;
		}
		System.out.println("Total slaves weight: " + totalSlavesWeight);

		// Delete the output directory after efficiency test
		deleteAndRemakeDirectoryInFileSystem(hdfsFileSystem, hdfs_out);
		
		// Delete old input files if they exist
		deleteHadoopInputFiles(slaves);
		
		// Create the hadoop input files for each mapper for each machine using efficinecies and available mappers
		int currSplit = 0;
		int currMapper = 0;
		for (String key : relativeSlaveEfficiencies.keySet()) {
			double efficiency = relativeSlaveEfficiencies.get(key);
			int mappers = mappersPerHost.get(key);
			double slaveWeight = ((double)mappers) * efficiency;
			
			int samples = (int) Math.round(((double) numSamples) * slaveWeight / totalSlavesWeight);
			
			double samplesPerMapperDouble = (((double) samples) / ((double) mappers));
			
			int samplesPerMapper = (int) samplesPerMapperDouble;
			int missingSamples = (int) Math.round((samplesPerMapperDouble - samplesPerMapper) * mappers);

			for (int j = 0; j < mappers; j++) {
				String outString = currSplit + "";
				currSplit++;
				for (int k = 1; k < samplesPerMapper; k++) {
					outString = outString + "_" + Integer.toString(currSplit);
					currSplit++;
				}
				if (missingSamples > 0) {
					outString = outString + "_" + Integer.toString(currSplit);
					currSplit++;
					missingSamples--;
				}
				System.out.println("Slave: " + key + " Mapper: " + j + " Splits: " + outString);
				indexes[currMapper] = outString;
				currMapper++;
				writeStringToFile(outString, key + "/" + Integer.toString(j));
			}
		}

		// Copy hadoop input files over to hdfs
		copyHadoopInputFilesToHDFS(hdfsFileSystem, slaves);
		
		//Delete output from previous potential runs
		FileUtils.deleteQuietly(new File("out_npairsj_ser"));
		
		// Execute the hadoop jobs
		executeHadoop(slaves);
		
		// Copy output files from hdfs to local
	    hdfsFileSystem.copyToLocalFile(true, hdfs_out, local);
	    
	    // Use generated results to calculate all the NPAIRS values
		numSlaves = numMappers;

		ppTrueClass_temp = new Matrix[numSlaves][totalNumSplitAnalyses];
		sqrdPredError_temp = new Matrix[numSlaves][totalNumSplitAnalyses];
		predClass_temp = new Matrix[numSlaves][totalNumSplitAnalyses];
		correctPred_temp = new Matrix[numSlaves][totalNumSplitAnalyses];
		ppAllClasses_temp = new Matrix[numSlaves][totalNumSplitAnalyses][2];
		corrCoeffs_temp = new Matrix[numSlaves];
		noisePattStdDev_temp = new Matrix[numSlaves];
		r2_temp = new Matrix[numSlaves][totalNumSplitAnalyses];

		for (int i = 0; i < numSlaves; i++) {
			if (indexes[i].length() == 0) {
				continue;
			}
			String j = Test.shortenPath(indexes[i]);
			FileInputStream fis = new FileInputStream(
					"out_npairsj_ser/avgCVScoresTest_" + j);
			ObjectInputStream ois = new ObjectInputStream(fis);

			try {
				avgCVScoresTest = avgCVScoresTest.plus((Matrix) ois
						.readObject());

			} catch (ClassNotFoundException e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			}
			ois.close();

			FileInputStream fis2 = new FileInputStream(
					"out_npairsj_ser/avgCVScoresTrain_" + j);
			ObjectInputStream ois2 = new ObjectInputStream(fis2);

			try {
				avgCVScoresTrain = avgCVScoresTrain.plus((Matrix) ois2
						.readObject());

			} catch (ClassNotFoundException e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			}
			ois2.close();

			FileInputStream fis3 = new FileInputStream(
					"out_npairsj_ser/avgNoisePattern_" + j);
			ObjectInputStream ois3 = new ObjectInputStream(fis3);
			FileInputStream fis4 = new FileInputStream(
					"out_npairsj_ser/avgSpatialPattern_" + j);
			ObjectInputStream ois4 = new ObjectInputStream(fis4);
			FileInputStream fis5 = new FileInputStream(
					"out_npairsj_ser/avgZScorePattern_" + j);
			ObjectInputStream ois5 = new ObjectInputStream(fis5);

			if (i == 0) {
				try {
					avgNoisePattern = (Matrix) ois3.readObject();

				} catch (ClassNotFoundException e) {
					// TODO Auto-generated catch block
					e.printStackTrace();
				}
				try {
					avgSpatialPattern = (Matrix) ois4.readObject();

				} catch (ClassNotFoundException e) {
					// TODO Auto-generated catch block
					e.printStackTrace();
				}
				try {
					avgZScorePattern = (Matrix) ois5.readObject();

				} catch (ClassNotFoundException e) {
					// TODO Auto-generated catch block
					e.printStackTrace();
				}
			} else {
				try {
					avgNoisePattern = avgNoisePattern.plus((Matrix) ois3
							.readObject());

				} catch (ClassNotFoundException e) {
					// TODO Auto-generated catch block
					e.printStackTrace();
				}
				try {
					avgSpatialPattern = avgSpatialPattern.plus((Matrix) ois4
							.readObject());

				} catch (ClassNotFoundException e) {
					// TODO Auto-generated catch block
					e.printStackTrace();
				}
				try {
					avgZScorePattern = avgZScorePattern.plus((Matrix) ois5
							.readObject());

				} catch (ClassNotFoundException e) {
					// TODO Auto-generated catch block
					e.printStackTrace();
				}
			}
			ois3.close();
			ois4.close();
			ois5.close();

			FileInputStream fis6 = new FileInputStream(
					"out_npairsj_ser/ppTrueClass_" + j);
			ObjectInputStream ois6 = new ObjectInputStream(fis6);

			try {
				ppTrueClass_temp[i] = (Matrix[]) ois6.readObject();

			} catch (ClassNotFoundException e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			}
			ois6.close();
			FileInputStream fis7 = new FileInputStream(
					"out_npairsj_ser/sqrdPredError_" + j);
			ObjectInputStream ois7 = new ObjectInputStream(fis7);

			try {
				sqrdPredError_temp[i] = (Matrix[]) ois7.readObject();

			} catch (ClassNotFoundException e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			}
			ois7.close();

			FileInputStream fis8 = new FileInputStream(
					"out_npairsj_ser/predClass_" + j);
			ObjectInputStream ois8 = new ObjectInputStream(fis8);

			try {
				predClass_temp[i] = (Matrix[]) ois8.readObject();

			} catch (ClassNotFoundException e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			}
			ois8.close();

			FileInputStream fis9 = new FileInputStream(
					"out_npairsj_ser/correctPred_" + j);
			ObjectInputStream ois9 = new ObjectInputStream(fis9);

			try {
				correctPred_temp[i] = (Matrix[]) ois9.readObject();

			} catch (ClassNotFoundException e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			}
			ois9.close();

			FileInputStream fis10 = new FileInputStream(
					"out_npairsj_ser/ppAllClasses_" + j);
			ObjectInputStream ois10 = new ObjectInputStream(fis10);

			try {
				ppAllClasses_temp[i] = (Matrix[][]) ois10.readObject();

			} catch (ClassNotFoundException e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			}
			ois10.close();

			FileInputStream fis11 = new FileInputStream("out_npairsj_ser/r2_"
					+ j);
			ObjectInputStream ois11 = new ObjectInputStream(fis11);

			try {
				r2_temp[i] = (Matrix[]) ois11.readObject();

			} catch (ClassNotFoundException e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			}
			ois11.close();

			FileInputStream fis12 = new FileInputStream(
					"out_npairsj_ser/corrCoeffs_" + j);
			ObjectInputStream ois12 = new ObjectInputStream(fis12);

			try {
				corrCoeffs_temp[i] = (Matrix) ois12.readObject();

			} catch (ClassNotFoundException e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			}
			ois12.close();

			FileInputStream fis13 = new FileInputStream(
					"out_npairsj_ser/noisePattStdDev_" + j);
			ObjectInputStream ois13 = new ObjectInputStream(fis13);

			try {
				noisePattStdDev_temp[i] = (Matrix) ois13.readObject();

			} catch (ClassNotFoundException e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			}
			ois13.close();
		}

		avgSpatialPattern = avgSpatialPattern
				.mult(1.0 / (double) (numSamples * 2));
		avgZScorePattern = avgZScorePattern.mult(1.0 / (double) numSamples);
		avgNoisePattern = avgNoisePattern.mult(1.0 / (double) numSamples);

		for (int splitNum = 0; splitNum < numSamples; ++splitNum) {

			int[] split1DataVols = splits[0][splitNum];
			int[] split2DataVols = splits[1][splitNum];

			++numAnalyses;

			if (setupParams.switchTrainAndTestSets) {

				++numAnalyses;
			}
			// Add r2 stats Matrix for current split half to r2 Matrix array.
			// if (computeR2) {
			// addCurrR2(splitNum);
			// }

			if (setupParams.cvaRun) {

				for (int vol : split1DataVols) {
					vCountTr[vol] += 1;
				}
				for (int vol : split2DataVols) {
					vCountTe[vol] += 1;
				}
				if (setupParams.switchTrainAndTestSets) {
					// split2CVAEvals.setRow(splitNum,
					// splitDataCVA2.getEvals());
					for (int vol : split2DataVols) {
						vCountTr[vol] += 1;
					}
					for (int vol : split1DataVols) {
						vCountTe[vol] += 1;
					}
				}
			}
			int index = 0;
			for (int i = 0; i < numSlaves; i++) {
				String[] array = indexes[i].split("_");
				if (Arrays.asList(array).contains(Integer.toString(splitNum))) {
					index = i;
				}
			}
			computePredStats2(numAnalyses, index);
			if (computeR2) {
				addCurrR2_2(splitNum, index);
			}
			if (splitNum == 0) {

				noisePattStdDev = new MatrixImpl(numSamples,
						noisePattStdDev_temp[index].numCols()).getMatrix();
				corrCoeffs = new MatrixImpl(numSamples,
						corrCoeffs_temp[index].numCols()).getMatrix();
			}

			corrCoeffs
					.setRow(splitNum, corrCoeffs_temp[index].getRow(splitNum));
			noisePattStdDev.setRow(splitNum,
					noisePattStdDev_temp[index].getRow(splitNum));

		} // end for loop through splits

		File directory = new File("out_npairsj_ser");

		// make sure directory exists
		if (!directory.exists()) {
			System.out.println("Directory does not exist.");
			System.exit(0);
		} else {
			try {
				delete(directory);
			} catch (IOException e) {
				e.printStackTrace();
				System.exit(0);
			}
		}
		// System.out.println("Done");

		// Finish calculating avg cv scores and evals:
		// divide avgCVScoresTrain/Test rows by corresponding vCounts.
		// If vCount = 0, then corresponding row in avgCVScoresTrain/Test
		// will be all zeros.
		for (int r = 0; r < setupParams.numVols; ++r) {
			if (vCountTr[r] > 0) {
				double[] avgCVSTrCurrRow = MLFuncs.divide(
						avgCVScoresTrain.getRowQuick(r), (double) vCountTr[r]);
				avgCVScoresTrain.setRowQuick(r, avgCVSTrCurrRow);
			}
			if (vCountTe[r] > 0) {
				double[] avgCVSTestCurrRow = MLFuncs.divide(
						avgCVScoresTest.getRowQuick(r), (double) vCountTe[r]);
				avgCVScoresTest.setRowQuick(r, avgCVSTestCurrRow);
			}
		}

		avgSplit1CVAEvals = split1CVAEvals.colMeans();
		if (setupParams.switchTrainAndTestSets) {
			avgSplit2CVAEvals = split2CVAEvals.colMeans();
		}

		// Reshape r2 data. Currently have a Matrix (nPCDims X nCVDims) for
		// each split analysis; want a Matrix (nSplitAnalyses X nPCDims) for
		// each CV Dim.
		if (computeR2) {
			r2 = reshapeR2();
		}
		// Pad prediction stats with -1's so all Matrices have maxNTestVols
		// columns
		padPredStats(totalNumSplitAnalyses);
	}

	// Alan: for deleting temp files
	public static void delete(File file) throws IOException {

		if (file.isDirectory()) {
			// directory is empty, then delete it
			if (file.list().length == 0) {
				file.delete();
			} else {
				// list all the directory contents
				String files[] = file.list();

				for (String temp : files) {
					// construct the file structure
					File fileDelete = new File(file, temp);
					// recursive delete
					delete(fileDelete);
				}

				// check the directory again, if empty then delete it
				if (file.list().length == 0) {
					file.delete();
				}
			}
		} else {
			// if file, then delete it
			file.delete();
		}
	}

	/**
	 * Rearranges r2 data from an array of (numSplitAnalyses) Matrices with dims
	 * (nPCDims rows X nCVDims cols) to an array of (nCVDims) Matrices with dims
	 * (nSplitAnalyses rows X nPCDims cols).
	 * 
	 * @return rearranged array of r2 Matrices
	 */
	private Matrix[] reshapeR2() {
		int nAnalyses = r2.length;
		int nPCDims = r2[0].numRows();
		int nCVDims = r2[0].numCols();
		Matrix[] r2PerCVDim = new Matrix[nCVDims];
		for (int cv = 0; cv < nCVDims; ++cv) {
			r2PerCVDim[cv] = new MatrixImpl(nAnalyses, nPCDims).getMatrix();
			for (int i = 0; i < nAnalyses; ++i) {
				r2PerCVDim[cv].setRow(i, r2[i].getColumn(cv));
			}
		}
		reshapedR2 = true;
		return r2PerCVDim;
	}

	/**
	 * Adds current split r2 stats to r2 Matrix
	 * 
	 * @param splitNum
	 *            - (0-relative) number of current data split
	 * 
	 */
	private void addCurrR2(int splitNum) {
		int analysisNum = splitNum;
		if (setupParams.switchTrainAndTestSets) {
			analysisNum *= 2;
		}
		if (setupParams.cvaRun) {
			r2[analysisNum] = splitDataCVA1.getR2();
			if (setupParams.switchTrainAndTestSets) {
				r2[analysisNum + 1] = splitDataCVA2.getR2();
			}
		}
	}

	private void addCurrR2_2(int splitNum, int index) {
		int analysisNum = splitNum;
		if (setupParams.switchTrainAndTestSets) {
			analysisNum *= 2;
		}
		if (setupParams.cvaRun) {
			r2[analysisNum] = r2_temp[index][analysisNum];
			if (setupParams.switchTrainAndTestSets) {
				r2[analysisNum + 1] = r2_temp[index][analysisNum + 1];
			}
		}
	}

	private void initR2Results(int totalNumSplitAnalyses) {
		r2 = new Matrix[totalNumSplitAnalyses];
	}

	private void initCVASplitResults(int numSamples) {
		// TODO: if split1,split2 have diff. number of pcs, the following
		// code will have to change
		int nCVDimsSplit1 = setupParams.cvaPCSet1.length;
		int nCVDims = Math.min(fullDataCVA.getNumCVDims(), nCVDimsSplit1);
		split1CVAEvals = new MatrixImpl(numSamples, nCVDims).getMatrix();
		avgSplit1CVAEvals = new double[nCVDims];
		avgCVScoresTrain = new MatrixImpl(setupParams.numVols, nCVDims)
				.getMatrix();
		avgCVScoresTest = new MatrixImpl(setupParams.numVols, nCVDims)
				.getMatrix();

		if (setupParams.switchTrainAndTestSets) {

			split2CVAEvals = new MatrixImpl(numSamples, nCVDims).getMatrix();
			avgSplit2CVAEvals = new double[nCVDims];
		}
	}

	private void initPredStats(int totalNoSplitAnalyses) {
		ppTrueClass = new Matrix[totalNoSplitAnalyses];
		sqrdPredError = new Matrix[totalNoSplitAnalyses];
		predClass = new Matrix[totalNoSplitAnalyses];
		correctPred = new Matrix[totalNoSplitAnalyses];
		ppAllClasses = new Matrix[totalNoSplitAnalyses][2];
	}

	/**
	 * Determine samples for each split.
	 * 
	 * @param random
	 *            If true, create uniformly random split samples (unless no. of
	 *            samples to be created is >= 90% of total possible no. of
	 *            samples; in this case, samples are created deterministically
	 *            even if 'random' is set to 'true'). If false, create split
	 *            samples deterministically.
	 * @see Resampler
	 */
	private void createSplits(boolean random) throws NpairsjException {

		if (setupParams.splits == null) {
			double sTime = System.currentTimeMillis();
			Resampler res = new Resampler(setupParams.getSplitObjLabels(),
					setupParams.getGroupLabels(),
					setupParams.numSplitObjInSplits, setupParams.numSplits);

			// 'splits' is array with dims [2][numSamples][] where
			// numSamples == min(numSplits, max poss splits)
			splits = res.generateSplits(random);
			if (debug) {
				double tTime = (System.currentTimeMillis() - sTime) / 1000;
				output.println("Total time creating splits: " + tTime + " s");
			}
		} else {
			splits = setupParams.splits;
		}
	}

	private void padPredStats(int totalNumAnalyses) {
		int maxNTestVols = 0;
		for (int i = 0; i < totalNumAnalyses; ++i) {
			maxNTestVols = Math.max(maxNTestVols, ppTrueClass[i].numCols());
		}

		for (int j = 0; j < totalNumAnalyses; ++j) {
			if (ppTrueClass[j].numCols() < maxNTestVols) {
				Matrix tmpPP = new MatrixImpl(2, maxNTestVols).getMatrix();
				tmpPP.set(-1);
				tmpPP.setSubMatrix(ppTrueClass[j], 0, 0);
				ppTrueClass[j] = tmpPP;

				Matrix tmpSPE = new MatrixImpl(2, maxNTestVols).getMatrix();
				tmpSPE.set(-1);
				tmpSPE.setSubMatrix(sqrdPredError[j], 0, 0);
				sqrdPredError[j] = tmpSPE;

				Matrix tmpPredCls = new MatrixImpl(2, maxNTestVols).getMatrix();
				tmpPredCls.set(-1);
				tmpPredCls.setSubMatrix(predClass[j], 0, 0);
				predClass[j] = tmpPredCls;

				Matrix tmpCorrPred = new MatrixImpl(2, maxNTestVols)
						.getMatrix();
				tmpCorrPred.set(-1);
				tmpCorrPred.setSubMatrix(correctPred[j], 0, 0);
				correctPred[j] = tmpCorrPred;

				int nModelDims = ppAllClasses[0][0].numCols();
				Matrix tmpPPAll = new MatrixImpl(maxNTestVols, nModelDims)
						.getMatrix();
				tmpPPAll.set(-1);
				tmpPPAll.setSubMatrix(ppAllClasses[j][0], 0, 0);
				ppAllClasses[j][0] = tmpPPAll;
				Matrix tmpPPAllNoP = new MatrixImpl(maxNTestVols, nModelDims)
						.getMatrix();
				tmpPPAllNoP.set(-1);
				tmpPPAllNoP.setSubMatrix(ppAllClasses[j][1], 0, 0);
				ppAllClasses[j][1] = tmpPPAllNoP;
			}
		}
	}

	private void computePredStats2(int numAnalyses, int index)
			throws NpairsjException {
		int currAnalysis = numAnalyses;
		if (setupParams.switchTrainAndTestSets) {
			currAnalysis--; // numAnalyses was incremented twice but curr
			// analysis is for first train set
		}
		ppTrueClass[currAnalysis - 1] = ppTrueClass_temp[index][currAnalysis - 1];
		sqrdPredError[currAnalysis - 1] = sqrdPredError_temp[index][currAnalysis - 1];
		predClass[currAnalysis - 1] = predClass_temp[index][currAnalysis - 1];
		correctPred[currAnalysis - 1] = correctPred_temp[index][currAnalysis - 1];
		ppAllClasses[currAnalysis - 1][0] = ppAllClasses_temp[index][currAnalysis - 1][0];
		ppAllClasses[currAnalysis - 1][1] = ppAllClasses_temp[index][currAnalysis - 1][1];

		if (setupParams.switchTrainAndTestSets) {
			currAnalysis++; // was decremented for first train set
			ppTrueClass[currAnalysis - 1] = ppTrueClass_temp[index][currAnalysis - 1];
			sqrdPredError[currAnalysis - 1] = sqrdPredError_temp[index][currAnalysis - 1];
			predClass[currAnalysis - 1] = predClass_temp[index][currAnalysis - 1];
			correctPred[currAnalysis - 1] = correctPred_temp[index][currAnalysis - 1];
			ppAllClasses[currAnalysis - 1][0] = ppAllClasses_temp[index][currAnalysis - 1][0];
			ppAllClasses[currAnalysis - 1][1] = ppAllClasses_temp[index][currAnalysis - 1][1];

		}

	}

	private void computePredStats(int numAnalyses, int[] split1DataVols,
			int[] split2DataVols, Matrix split1CVSTrain, Matrix split1CVSTest,
			Matrix split2CVSTrain, Matrix split2CVSTest)
			throws NpairsjException {
		double sTime = 0;
		double tTime = 0;
		if (debug) {
			output.print("Calculating Prediction Stats "
					+ "for 1st split half... ");
			sTime = System.currentTimeMillis();
		}
		PredictionStats predStats1 = new PredictionStats(split1CVSTrain,
				split2CVSTest, split1DataVols, split2DataVols,
				setupParams.getClassLabels());
		if (debug) {
			tTime = (System.currentTimeMillis() - sTime) / 1000;
			output.println("[" + tTime + "]");
		}

		// add to cumulative prediction stats
		int currAnalysis = numAnalyses;
		if (setupParams.switchTrainAndTestSets) {
			currAnalysis--; // numAnalyses was incremented twice but curr
			// analysis is for first train set
		}
		ppTrueClass[currAnalysis - 1] = new MatrixImpl(
				predStats1.getPPTrueClass()).getMatrix();
		sqrdPredError[currAnalysis - 1] = new MatrixImpl(
				predStats1.getSqrdPredError()).getMatrix();
		predClass[currAnalysis - 1] = new MatrixImpl(predStats1.getPredClass())
				.getMatrix();
		correctPred[currAnalysis - 1] = new MatrixImpl(
				predStats1.getCorrectPred()).getMatrix();
		ppAllClasses[currAnalysis - 1][0] = new MatrixImpl(
				predStats1.getPPAllClasses(0)).getMatrix();
		ppAllClasses[currAnalysis - 1][1] = new MatrixImpl(
				predStats1.getPPAllClasses(1)).getMatrix();

		PredictionStats predStats2 = null;
		if (setupParams.switchTrainAndTestSets) {
			currAnalysis++; // was decremented for first train set
			if (debug) {
				output.print("Calculating Prediction Stats "
						+ "for 2nd split half... ");
				sTime = System.currentTimeMillis();
			}
			predStats2 = new PredictionStats(split2CVSTrain, split1CVSTest,
					split2DataVols, split1DataVols,
					setupParams.getClassLabels());
			if (debug) {
				tTime = (System.currentTimeMillis() - sTime) / 1000;
				output.println("[" + tTime + "]");
			}

			ppTrueClass[currAnalysis - 1] = new MatrixImpl(
					predStats2.getPPTrueClass()).getMatrix();
			sqrdPredError[currAnalysis - 1] = new MatrixImpl(
					predStats2.getSqrdPredError()).getMatrix();
			predClass[currAnalysis - 1] = new MatrixImpl(
					predStats2.getPredClass()).getMatrix();
			correctPred[currAnalysis - 1] = new MatrixImpl(
					predStats2.getCorrectPred()).getMatrix();
			ppAllClasses[currAnalysis - 1][0] = new MatrixImpl(
					predStats2.getPPAllClasses(0)).getMatrix();
			ppAllClasses[currAnalysis - 1][1] = new MatrixImpl(
					predStats2.getPPAllClasses(1)).getMatrix();
		}
	}

	private void savePredictionStats(boolean saveLotsOfFiles)
			throws IOException {

		// Matrix[totalNumAnalyses] ppTrueClass
		// " sqrdPredError
		// " predClass
		// " correctPred
		// Matrix[totalNumAnalyses][2] ppAllClasses
		// - each Matrix is 2 X num test vols except
		// for ppAllClasses Matrices, which are
		// num test vols X num classes

		int nAnalyses = ppTrueClass.length;
		int maxNTestVols = 0; // not all test sets have same number of vols
		for (int i = 0; i < nAnalyses; ++i) {
			maxNTestVols = Math.max(maxNTestVols, ppTrueClass[i].numCols());
		}
		// always save true class post probs with priors for all split halves
		// and test vols
		// in text file as 2D array, even if not saving lots of files
		Matrix ppTrueClsPriors = new MatrixImpl(nAnalyses, maxNTestVols)
				.getMatrix();
		for (int i = 0; i < nAnalyses; ++i) {
			ppTrueClsPriors.setRow(i, ppTrueClass[i].getRow(0));
		}
		String modelType = "";
		if (setupParams.cvaRun) {
			modelType = "CVA";
		}
		String ppTruePriorsFilename = setupParams.resultsFilePrefix + "."
				+ modelType + ".SUMM.PP.ppTruePriors";
		ppTrueClsPriors.printToFile(ppTruePriorsFilename, "IDL");

		if (saveLotsOfFiles) { // save all the prediction stats, not just the
								// true class post probs

			int nModelDims = ppAllClasses[0][0].numCols();
			double[][][] ppTrueCls3D = new double[nAnalyses][2][maxNTestVols];
			double[][][] sqrdPredErr3D = new double[nAnalyses][2][maxNTestVols];
			double[][][] predCls3D = new double[nAnalyses][2][maxNTestVols];
			double[][][] corrPred3D = new double[nAnalyses][2][maxNTestVols];
			double[][][] ppAllClsPriors3D = new double[nAnalyses][maxNTestVols][nModelDims];
			double[][][] ppAllClsNoPriors3D = new double[nAnalyses][maxNTestVols][nModelDims];

			for (int i = 0; i < nAnalyses; ++i) {
				ppTrueCls3D[i] = ppTrueClass[i].toArray();
				sqrdPredErr3D[i] = sqrdPredError[i].toArray();
				predCls3D[i] = predClass[i].toArray();
				corrPred3D[i] = correctPred[i].toArray();
				ppAllClsPriors3D[i] = ppAllClasses[i][0].toArray();
				ppAllClsNoPriors3D[i] = ppAllClasses[i][1].toArray();
			}

			// save in .img/hdr 3D volume format
			String ppTrueFilename = setupParams.resultsFilePrefix + "."
					+ modelType + ".SUMM.PP.pp";
			String speFilename = setupParams.resultsFilePrefix + "."
					+ modelType + ".SUMM.PP.spe";
			String predClsFilename = setupParams.resultsFilePrefix + "."
					+ modelType + ".SUMM.PP.predCls";
			String corrPFilename = setupParams.resultsFilePrefix + "."
					+ modelType + ".SUMM.PP.pred";
			String ppAllPriorsFilename = setupParams.resultsFilePrefix + "."
					+ modelType + ".SUMM.PP.ppAllPriors";
			String ppAllNoPriorsFilename = setupParams.resultsFilePrefix + "."
					+ modelType + ".SUMM.PP.ppAllNoPriors";

			// TODO: consider xyz ordering in these volumes (although they're
			// not images, we still
			// need to understand how the data is ordered in the file)
			int datatype = 16; // 32-bit float
			NiftiIO.writeVol(ppTrueCls3D, datatype, ppTrueFilename);
			NiftiIO.writeVol(sqrdPredErr3D, datatype, speFilename);
			NiftiIO.writeVol(predCls3D, datatype, predClsFilename);
			NiftiIO.writeVol(corrPred3D, datatype, corrPFilename);
			NiftiIO.writeVol(ppAllClsPriors3D, datatype, ppAllPriorsFilename);
			NiftiIO.writeVol(ppAllClsNoPriors3D, datatype,
					ppAllNoPriorsFilename);

			// if (debug) {
			// ppTrueClass[0].transpose().printToFile(setupParams.resultsFilePrefix
			// + ".CVA.SUMM.PP.0", "IDL");
			// sqrdPredError[0].transpose().printToFile(setupParams.resultsFilePrefix
			// + ".CVA.SUMM.spe.0", "IDL");
			// }
		}
	}

	public NpairsjSetupParams getSetupParams() {
		return setupParams;
	}

	public Matrix getCorrCoeffs() {
		return corrCoeffs;
	}

	// public static void writeVol(double[][][] vol3D, String filename) throws
	// IOException, FileNotFoundException {
	// Nifti1Dataset niftiDS = new Nifti1Dataset();
	// niftiDS.setHeaderFilename(filename);
	// niftiDS.setDataFilename(filename);
	// niftiDS.setDatatype((short)64);
	// niftiDS.setDims((short)3, (short)vol3D[0][0].length,
	// (short)vol3D[0].length, (short)vol3D.length,
	// (short)0, (short)0, (short)0, (short)0);
	// niftiDS.writeHeader();
	// niftiDS.writeVol(vol3D, (short)0);
	// }

	/***************************************************************************************
	 * Private helper methods:
	 *************************************************************************************** 
	 */

	/**
	 * Returns new Matrix containing test CV scores, i.e., cvaTest data
	 * projected onto cvaTrain CVA eigenimages, for vols in test data set. Vols
	 * not incl. in test data set have cv scores set to zero in returned Matrix.
	 * Dim CVA eigenimages - numDataCols (nVox, nFeatSelDims or nPCDimsForCVA) X
	 * nCVDims Dim testData - numTestDataVols X numDataCols
	 * 
	 * @param cvaTrain
	 *            - cva of training data
	 * @param testDataVols
	 *            - indices of vols (rows) of test data in full data set
	 */
	private Matrix getTestCVScores(CVA cvaTrain, int[] testDataVols) {
		Matrix testData = null;

		if (setupParams.initFeatSelect) {
			testData = dataLoader.getFeatSelData().subMatrixRows(testDataVols);
		} else {
			testData = dataLoader.getOrigData().subMatrixRows(testDataVols);
		}
		testData = testData.meanCentreColumns();

		Matrix cvsTest = cvaTrain.calcTestCVScores(testData);
		Matrix cvsTestPadded = cvsTest.zeroPadRows(setupParams.numVols,
				testDataVols);
		return cvsTestPadded;
	}

	/**
	 * Returns new Matrix containing training CV scores. Vols not incl. in
	 * training data set have cv scores set to zero in returned Matrix. Dim CVA
	 * eigenimages - numDataCols (nVox or nFeatSelDims) X nCVDims Dim
	 * trainingData - numTrainDataVols X numDataCols
	 * 
	 * @param cvaTrain
	 *            - cva of training data
	 * @param trainDataVols
	 *            - indices of vols (rows) of training data in full data set
	 */
	private Matrix getTrainCVScores(CVA cvaTrain, int[] trainDataVols) {
		Matrix cvsTr = cvaTrain.getCVScores();
		Matrix cvsTrPadded = cvsTr.zeroPadRows(setupParams.numVols,
				trainDataVols);
		return cvsTrPadded;
	}

	public Matrix[] getR2() {
		return r2;
	}

	private void saveSplitVolInfo() throws IOException {
		// Save splits vol info into single read_matrix.pro-format file
		// (compatible with IDL npairs SETUP .vols file)
		// NOTE that volume indices are 1-RELATIVE in IDL-compatible file
		int numSamples = splits[0].length;
		// reorder 'splits' so first row == first split half sample 1,
		// second row == 2nd split half sample 1,
		// third row == first split half sample 2,
		// fourth row == 2nd split half sample 2,
		// ... etc.
		// store result in tmpSplits
		// NOTE that if split halves containing less than max no. of vols
		// across all split halves will be zero-padded so all rows have same
		// no. of elements
		int[][] tmpSplits = new int[2 * numSamples][];
		int maxNumVols = 0;
		for (int s = 0; s < numSamples; ++s) {
			int currMax = Math.max(splits[0][s].length, splits[1][s].length);
			maxNumVols = Math.max(maxNumVols, currMax);
		}
		for (int s = 0; s < numSamples; ++s) {

			int[] zeropadSplits0 = new int[maxNumVols];
			for (int i = 0; i < splits[0][s].length; ++i) {
				zeropadSplits0[i] = splits[0][s][i] + 1;
			}
			for (int j = splits[0][s].length; j < maxNumVols; ++j) {
				zeropadSplits0[j] = 0;
			}
			int[] zeropadSplits1 = new int[maxNumVols];
			for (int i = 0; i < splits[1][s].length; ++i) {
				zeropadSplits1[i] = splits[1][s][i] + 1;
			}
			for (int j = splits[1][s].length; j < maxNumVols; ++j) {
				zeropadSplits1[j] = 0;
			}

			tmpSplits[2 * s] = zeropadSplits0;
			tmpSplits[2 * s + 1] = zeropadSplits1;
		}

		String splitsInfoSaveFilename = setupParams.resultsFilePrefix + ".vols";
		NpairsjIO.printToIDLFile(tmpSplits, splitsInfoSaveFilename);
	}

	private void saveAvgSpatPattern(String format) {
		if (format.toUpperCase().equals("IDL")) {
			String avgSpatPattFilename = setupParams.resultsFilePrefix
					+ ".CVA.SUMM.AVG.eigim";
			avgSpatialPattern.printToFile(avgSpatPattFilename, format);
		} else {
			throw new IllegalArgumentException("Input format \'" + format
					+ "\' not " + "implemented.");
		}
	}

	private void saveZScoreAvgPatt(String format) {
		if (format.toUpperCase().equals("IDL")) {
			String avgZScorePattFilename = setupParams.resultsFilePrefix
					+ ".CVA.SUMM.AVG.rSPM-Z";
			avgZScorePattern.printToFile(avgZScorePattFilename, format);
		} else {
			throw new IllegalArgumentException("Input format \'" + format
					+ "\' not " + "implemented.");
		}
	}

	private void saveNoiseAvgPatt(String format) {
		if (format.toUpperCase().equals("IDL")) {
			String avgNoisePattFilename = setupParams.resultsFilePrefix
					+ ".CVA.SUMM.AVG.noise";
			avgNoisePattern.printToFile(avgNoisePattFilename, format);
		} else {
			throw new IllegalArgumentException("Input format \'" + format
					+ "\' not " + "implemented.");
		}
	}

	private void saveNoiseStdDev(String format) {
		if (format.toUpperCase().equals("IDL")) {
			String noiseStdDevFilename = setupParams.resultsFilePrefix
					+ ".CVA.SUMM.noise-sd";
			noisePattStdDev.printToFile(noiseStdDevFilename, format);
		} else {
			throw new IllegalArgumentException("Input format \'" + format
					+ "\' not " + "implemented.");
		}
	}

	private void saveCorrCoeffs(String format) {
		if (format.toUpperCase().equals("IDL")) {
			String corrCoeffsFilename = setupParams.resultsFilePrefix
					+ ".CVA.SUMM.CC";
			corrCoeffs.printToFile(corrCoeffsFilename, format);
		} else {
			throw new IllegalArgumentException("Input format \'" + format
					+ "\' not " + "implemented.");
		}
	}

	private void saveAvgCVScores(String format) {
		if (format.toUpperCase().equals("IDL")) {
			String saveCVSTrain = setupParams.resultsFilePrefix
					+ ".CVA.SUMM.AVG.CV-TR";
			avgCVScoresTrain.printToFile(saveCVSTrain, format);
			String saveCVSTest = setupParams.resultsFilePrefix
					+ ".CVA.SUMM.AVG.CV-TE";
			avgCVScoresTest.printToFile(saveCVSTest, format);
		} else {
			throw new IllegalArgumentException("Input format \'" + format
					+ "\' not " + "implemented.");
		}
	}

	/**
	 * Save PCA results for current split half 'splitNo'. (Split numbers are
	 * 1-relative: 1, 2)
	 */
	private void savePCASplitResults(int splitNum, int splitHalf)
			throws NpairsjException {

		String pcaSavePref = setupParams.resultsFilePrefix;
		if (!setupParams.pcEigimsToBigSpace) {
			pcaSavePref += ".InitFSpace";
		}
		if (splitHalf == 1) {
			splitDataPCA1.savePCAResultsIDL(pcaSavePref, null, true, splitNum,
					splitHalf);
			// save denoised (i.e. PCA dim-reduced) input data
			// (in orig img space)
			// TODO: determine if one would ever want split image
			// data saved (doesn't seem likely)
			// if (setupParams.saveDataPostPCA) {
			// splitDataPCA1.saveDataPostPCA(dataLoader);
			// }
		} else {
			splitDataPCA2.savePCAResultsIDL(pcaSavePref, null, true, splitNum,
					splitHalf);
			// save denoised (i.e. PCA dim-reduced) input data
			// (in orig img space)
			// if (setupParams.saveDataPostPCA) {
			// splitDataPCA2.saveDataPostPCA(dataLoader);
			// }
		}
	}

	// /**Only rotates the input pca eigenimage dimensions back into original
	// space; otherwise would take
	// * prohibitive length of time for large data.
	// * NOTE: pca pc scores and eigenvalues are also truncated to include only
	// input pca dims.
	// * @param pca
	// * @param pcaDims
	// * @param invProjectedData TODO
	// * @param origData TODO
	// */
	// private void rotateEigimsToOrigSpace(PCA pca, int[] pcaDims, Matrix
	// invProjectedData, Matrix origData) {
	//
	// // project pca eigenimages back onto inverted feature-selection
	// // projection Matrix, i.e., feat-sel eigenimages (note that inverted
	// // proj. matrix == transpose of proj. matrix == Vt)
	//
	// // P = MV ==> PVt = M ==> Vt = (invP)M == inverted feat-sel
	// // proj. Matrix (Vt), i.e. feat-sel eigims == transpose of proj. Matrix
	// V.
	// // Vt ( == invSVDEigims, e.g.,) == too large to store, hence
	// // store invP = invFeatSelData instead to reconstruct Vt on the fly.
	// // Vt = (invP)M ==> PVt = P(invP)M ==> P1Vt = P1(invP)M = M1
	// // where P1 = PCA eigenimages in rows, in this case, and M1 =
	// // PCA eigims in rows proj. back into orig. voxel space.
	// // given size P1 is reducDataDims X reducDataDims
	// // and size (invP) is reducDataDims X data.numRows(),
	// // and size M is data.numRows() X origDataDims,
	// // want M1t = origDataDims X reducDataDims();
	// // TODO: Want to retain only significant dimensions of PCs!
	// // (see IDL code for comparison)
	//
	// // size P1(invP) is reducDataDims X data.numRows():
	// // Matrix P1invP =
	// pca.getEvects(pcaDims).transpose().mult(dataLoader.getInvFeatSelData());
	// Matrix P1invP =
	// pca.getEvects(pcaDims).transpose().mult(invProjectedData);
	//
	// // instead of multiplying P1 by invPM
	// // == (reducDataDims X reducDataDims) * (reducDataDims X origDataDims),
	// // multiply P1(invP) by M
	// // == (reducDataDims X data.numRows()) * (data.numRows() X origDataDims)
	// // == fewer calculations
	// // Matrix voxSpacePCAEvects = P1invP.mult(dataLoader.getOrigData());
	// Matrix voxSpacePCAEvects = P1invP.mult(origData);
	//
	// pca.setEvects(voxSpacePCAEvects.transpose());
	// pca.truncateEvalsAndScores(pcaDims);
	//
	// }

	// private void rotateEigimsToOrigSpace(CVA cva, Matrix invProjectedData,
	// Matrix origData) {
	// // project cva results back onto inverted feature-selection
	// // projection Matrix
	// // (see PCA.rotateEigimsToOrigSpace(...) documentation for algebraic
	// details)
	//
	// // double sTime = System.currentTimeMillis();
	// Matrix P1invP = cva.getEigims().transpose().mult(invProjectedData);
	// // if (debug) {
	// // double tTime = (System.currentTimeMillis() - sTime) / (double)1000;
	// // output.println("Total time calc. A = cvEigim*iFeatSelData (rot): " +
	// tTime + " s");
	// // output.println("Dims A: " + P1invP.numRows() + " X " +
	// P1invP.numCols());
	// // }
	// // sTime = System.currentTimeMillis();
	// Matrix voxSpaceCVAEigims = P1invP.mult(origData);
	// // if (debug) {
	// // double tTime = (System.currentTimeMillis() - sTime) / (double)1000;
	// // output.println("Total time calc. A*origData: " + tTime + " s");
	// // }
	// cva.setEigims(voxSpaceCVAEigims.transpose());
	//
	// }

	public boolean useCondsAsClasses() {
		return setupParams.useCondsAsClasses;
	}

	public NpairsDataLoader getDataLoader() {
		return dataLoader;
	}

	// public void finalize() throws Throwable {
	// System.out.println("Npairsj killed.");
	//
	// super.finalize();
	// }
	
	
	// Determines the reproducibility The results of the reproducibility stats are saved in a file that contains
	// double values equivalent to the number of splits. All of them can be average
	// to acquire an overall value for r.
	private double getPredictionValue() throws IOException {
		if (!setupParams.cvaRun) {
			try {
				throw new Exception ("Why is setupParams.cvaRun false? Does not account for this case.");
			} catch (Exception e) {
				e.printStackTrace();
				System.exit(-1);
			}
		}
		String fileName = setupParams.resultsFilePrefix + ".CVA.SUMM.PP.ppTruePriors";
		BufferedReader in = new BufferedReader(new FileReader(fileName));
		String line = in.readLine();
		// Assumes that first line is of the expected structure		
		String[] dimensions = line.split(" ");
//		int maxNTestVols =Integer.parseInt(dimensions[0]);
		int numSplitHalfs = Integer.parseInt(dimensions[1]);
		
		double predictability = 0;
		
		for (int i = 0; i < numSplitHalfs; i++) {
			String[] nums = in.readLine().split(" ");
			double p = 0;
			int j;
			for (j = 0; j < nums.length; j++) {
				double num = Double.parseDouble(nums[j]);
				// if the value is -1.0, don't take it into account and average the prob independent of it.
				// This happens because maxNTestVols is the max among all splithalfs, while not all tests
				// use all of it.
				if (num < 0) {
					break;
				}
				p += num;
			}
			p = p / ((double) j);
			predictability += p;
		}
		predictability = predictability / ((double) numSplitHalfs);
		
		in.close();
				
		return predictability;
	}
	
	// The results of the reproducibility stats are saved in a file that contains
	// double values equivalent to the number of splits. All of them can be average
	// to acquire an overall value for r.
	private double getReproducibilityValue() throws IOException {
		String fileName = setupParams.resultsFilePrefix + ".CVA.SUMM.CC";
		BufferedReader in = new BufferedReader(new FileReader(fileName));
		String line = in.readLine();
		// Assumes that first line is of the expected structure
		String[] dimensions = line.split(" ");
//		int numsPerSplit =Integer.parseInt(dimensions[0]);
		int numSplits = Integer.parseInt(dimensions[1]);

		double reproducibility = 0;
		for (int i = 0; i < numSplits; i++) {
			String[] nums = in.readLine().split(" ");
			
			double tempReproducibility = 0;
			for (int j = 0; j < nums.length; j++) {
				tempReproducibility += Double.parseDouble(nums[j]);
			}
			tempReproducibility = tempReproducibility / ((double) nums.length);
			reproducibility += tempReproducibility;
		}
		reproducibility = reproducibility / ((double) numSplits);
		
		in.close();
		
		return reproducibility;
	}
}
